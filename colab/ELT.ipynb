{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2657647",
   "metadata": {},
   "source": [
    "# ELT (Extract, Load, Transform) - Practical Exercises\n",
    "In this notebook, we will explore the ELT process using Python and some common data manipulation libraries such as pandas and SQLAlchemy.\n",
    "\n",
    "### Objectives\n",
    "- Understand the differences between ETL and ELT.\n",
    "- Perform basic data extraction, loading, and transformation using Python.\n",
    "- Work with a sample dataset to gain hands-on experience with ELT operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8cb5a",
   "metadata": {},
   "source": [
    "## 1. Extract\n",
    "The extraction step involves fetching data from a source, which can be an API, a database, or a file. Here, we'll work with a CSV file as the data source.\n",
    "\n",
    "**Exercise**: Load a sample CSV file into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load a sample CSV file (replace with a file path or URL if necessary)\n",
    "url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/airtravel.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffed8d0",
   "metadata": {},
   "source": [
    "## 2. Load\n",
    "Once the data is extracted, it needs to be loaded into a destination, such as a database or a data warehouse. For this exercise, we'll simulate this by loading the DataFrame into an in-memory SQLite database using SQLAlchemy.\n",
    "\n",
    "**Exercise**: Load the DataFrame into an SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an in-memory SQLite database and load the DataFrame into a table\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "df.to_sql('airtravel', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "# Display the tables in the database to confirm the load operation\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8c731",
   "metadata": {},
   "source": [
    "## 3. Transform\n",
    "The transform step involves cleaning, aggregating, or otherwise modifying the data to fit the desired format. This step can include operations such as filtering, grouping, or joining multiple datasets.\n",
    "\n",
    "**Exercise**: Perform a transformation on the data by filtering records where air travel in '1958' was greater than 300 and calculate the average number of passengers for these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a96543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "transformed_df = df[df['1958'] > 300]\n",
    "average_passengers = transformed_df['1958'].mean()\n",
    "\n",
    "# Display the transformed data and the average\n",
    "transformed_df, average_passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af466c",
   "metadata": {},
   "source": [
    "## 4. Putting it All Together\n",
    "Now that you've seen each of the individual steps, try putting them together in a single function.\n",
    "\n",
    "**Exercise**: Create a function `elt_pipeline` that:\n",
    "1. Extracts data from a CSV file.\n",
    "2. Loads it into an SQLite database.\n",
    "3. Transforms the data by filtering for a specified year and calculating the average number of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elt_pipeline(csv_url, year_column, passenger_threshold):\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    # Step 1: Extract\n",
    "    df = pd.read_csv(csv_url)\n",
    "\n",
    "    # Step 2: Load\n",
    "    engine = create_engine('sqlite:///:memory:')\n",
    "    df.to_sql('airtravel', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "    # Step 3: Transform\n",
    "    transformed_df = df[df[year_column] > passenger_threshold]\n",
    "    average_passengers = transformed_df[year_column].mean()\n",
    "\n",
    "    return transformed_df, average_passengers\n",
    "\n",
    "# Run the ELT pipeline with the sample CSV\n",
    "elt_pipeline(url, '1958', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51728b",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "You've now gone through a basic ELT pipeline using Python. This notebook demonstrated how to extract data from a source, load it into a database, and perform transformations. You can expand upon this by connecting to external databases, adding more complex transformations, and integrating with BI tools."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
