{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Architecture\n",
    "\n",
    "In the lecture we learned, that there is _no free lunch_ and different databases are designed for different purposes and therefore have advantages and disadvantages. In this python notebook, we want to gather some practical experiences\n",
    "using databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices in Action\n",
    "\n",
    "### Background Story\n",
    "\n",
    "A fictional company *QuickMart* is a rapidly growing online store for electronics with thousands of products across various categories. With a vast customer base, QuickMart relies on fast and efficient product searches to keep up with users’ demands and to ensure low latencies. Every time a customer searches for an item, such as “Bluetooth Headphones,” the system needs to retrieve matching products from thousands of entries in real-time.\n",
    "\n",
    "For simplicity, QuickMart’s search function uses exact product names, making it critical to match customer queries to the Product Title column in the database. However, with a growing catalog, finding an exact match by sequentially scanning every product title is increasingly time-consuming. Developers from QuickMart reached out to you, since you are an expert in optimizing database accesses.\n",
    "\n",
    "### Overview\n",
    "1. **Naive Approach**: We are starting slowly and implementing some functions which simulates the naive approach to access the products in a database. This is how Quickmart did up to now. Since we are already familiar with _pandas_, we are using pandas as the database of our choice to apply our operations.\n",
    "2. **Quick Access using the simulation of an index**: Using the naive approach, we have gained a feeling about efficiency and up to how many requests this might scale. However, Quickmart grew rapidly in the past, therefore we need another strategy for the access into the database. Here we will simulate the index access to get an impression about what indices are capable of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/styx0r/msb_lecture_data_management_and_business_intelligence/refs/heads/main/data/pricerunner_aggregate.csv\"\n",
    "products = pd.read_csv(url)\n",
    "\n",
    "def get_product(product_title, products):\n",
    "    # Given a product_title and the products, please find the row,\n",
    "    # where the product_title equals the column 'Product Title' in products\n",
    "    return products[products['Product Title'] == product_title]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Given the get_product function, the product and a product_title,\n",
    "# how long does it take to get the information of a product?\n",
    "# Note: we can use the cell magic function %%time to measure the time of the execution of this cell.\n",
    "\n",
    "# Thing about what the database is doing in order to return one product. Is it efficient?\n",
    "\n",
    "product_title = \"aeg rcb83724vw 60/40 fridge freezer white\"\n",
    "\n",
    "get_product(product_title, products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the backend of Quickmart, the get_product function is called not just once,\n",
    "# but hundreds of times per second. So next we want to measure the\n",
    "# time it takes to call the process k (e.g. 2000) times.\n",
    "# So first of all we need to randomly select k products and save them to a list.\n",
    "\n",
    "k = 2000\n",
    "\n",
    "# Please select randomly k elements from the 'Product Title' column of products, which\n",
    "# which we will user later to access the products k times.\n",
    "# Since products can be accessed multiple time, please sample k products randomly\n",
    "# with replacement.\n",
    "\n",
    "random_product_titles = products['Product Title'].sample(k, replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Here we want to measure the time when we call the get_products function with\n",
    "# our samples k random products.\n",
    "\n",
    "# Please iteratively use the get_product function to get all products from random_product_titles.\n",
    "# Hint: You can use a for loop. Alternatively there is an apply function which you might use.\n",
    "\n",
    "random_products = random_product_titles.apply(lambda product_title: get_product(product_title, products))\n",
    "\n",
    "# How long does it take for k = 2000?\n",
    "# How long does it take for 1000, for 2000, ... ? How does it scale regarding time?\n",
    "\n",
    "# Hint: When asking about scaling, we want to find out how much is the increase of a unit (e.g. time)\n",
    "# when the complexity of the problem increases (e.g. k).\n",
    "# Imagine you plot k against the time. If it is a straight line, it's called a linear scale.\n",
    "# If for example it is a logarithmic or an exponential function it is a log scale or exponential scale, respectively.\n",
    "\n",
    "# To solve this task, we need to change the k from the cell before and run this cell and remember the times it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Additional Task: Automate the process we just did manually.\n",
    "# Please run it for incremental sample sizes k and create a plot.\n",
    "# The x axis should be the sample size and the y axis the time it took to get the products.\n",
    "\n",
    "# Hint: to measure time between lines of code in python, you can use the\n",
    "# time library. After importing time, time.process_time() returns a timestamp\n",
    "# in seconds at this very point in time.\n",
    "\n",
    "# Define the different sample sizes (k values) you want to test\n",
    "sample_sizes = [1, 10, 100, 1000, 2000, 3000, 4000, 5000, 10000, 20000]\n",
    "times = []  # List to store time taken for each sample size\n",
    "\n",
    "# Loop through each sample size\n",
    "for k in sample_sizes:\n",
    "    # Take a sample of 'Product Title' column with size k\n",
    "    random_product_titles = products['Product Title'].sample(k, replace=True)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Apply the get_product function on each product title in the sample\n",
    "    random_products = random_product_titles.apply(lambda product_title: get_product(product_title, products))\n",
    "\n",
    "    # Record the end time and calculate the elapsed time\n",
    "    end_time = time.perf_counter()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "# Plotting the times against the sample sizes\n",
    "plt.plot(sample_sizes, times, marker='o')\n",
    "plt.xlabel('Number of Samples (k)')\n",
    "plt.ylabel('Time Taken (seconds)')\n",
    "plt.title('Time vs. Sample Size')\n",
    "plt.show()\n",
    "\n",
    "# Does the plot support your thoughts regarding the scale?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quick Access using the simulation of an index\n",
    "\n",
    "Quickmart has grown a lot recently. They have about 2000 product requests every second. They realized that the shop got really slow and customers started to complain. They have the feeling that they could grow even faster but customers are annoyed of the slow shop and therefore prefer to buy electronics somewhere else. What should they do? You remember a very nice module called 'BI and Data Management' during your masters and there you've heard about indices and that those might help to improve the reading times when accessing the database. So let's get to work...\n",
    "\n",
    "You remember, the first ingredient is the fast access of a database if we use the row number / index. In pandas we can do that by using loc. E.g. to get the 42th entry of the datatable we can call products.loc[42].\n",
    "It's also possible to access multiple row numbers at once by using an array, e.g. like this: products.loc[[23,42]]\n",
    "\n",
    "The second ingredient that we need is an auxiliary table which allows us to efficiently \n",
    "get the row numbers of the products table given a product title. In the lecture we discussed the binary search.\n",
    "However, in this exercise we would like to use a dictionary from python. In a python dictionary we have the possible to very efficiently map a given string to a data type of your choice. In our case, we just want to map the product titles to the row numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please create the auxiliary table, which can be used to efficiently access the original products table.\n",
    "products_aux = {}\n",
    "for index, product_title in products['Product Title'].items():\n",
    "    products_aux[product_title] = index\n",
    "\n",
    "# Pythonic short version using dictionary comprehension\n",
    "products_aux = {product_title: index for index, product_title in products['Product Title'].items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the auxiliary table, we design a faster version of the get_products function\n",
    "# by utilizing this table.\n",
    "\n",
    "def get_product_fast(product_title, products_aux, products):\n",
    "    row_index = products_aux[product_title]\n",
    "    return products.loc[row_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's the moment of truth, was our effort worth it? Let's measure the time for 10000 hits!\n",
    "k = 10000\n",
    "\n",
    "# So let's create the random product titles again.\n",
    "random_product_titles = products['Product Title'].sample(k, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Measure the time!\n",
    "random_product_titles.apply(lambda product_title: get_product_fast(product_title, products_aux, products))\n",
    "\n",
    "# How long does it take for 1000, 10000, 100000, ...? How does it scale regarding time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Additional Task: Again, automate the process we just did manually.\n",
    "# Please run it for incremental sample sizes k and create a plot.\n",
    "# The x axis should be the sample size and the y axis the time it took to get the products.\n",
    "\n",
    "# Hint: to measure time between lines of code in python, you can use the\n",
    "# time library. After importing time, time.process_time() returns a timestamp\n",
    "# in seconds at this very point in time.\n",
    "\n",
    "# Define the different sample sizes (k values) you want to test\n",
    "sample_sizes = [1, 10, 100, 1000, 2000, 3000, 4000, 5000, 10000, 20000, 50000, 100000, 150000, 200000]\n",
    "times = []  # List to store time taken for each sample size\n",
    "\n",
    "# Loop through each sample size\n",
    "for k in sample_sizes:\n",
    "    # Take a sample of 'Product Title' column with size k\n",
    "    random_product_titles = products['Product Title'].sample(k, replace=True)\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Apply the get_product function on each product title in the sample\n",
    "    random_products = random_product_titles.apply(lambda product_title: get_product_fast(product_title, products_aux, products))\n",
    "\n",
    "    # Record the end time and calculate the elapsed time\n",
    "    end_time = time.perf_counter()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "# Plotting the times against the sample sizes\n",
    "plt.plot(sample_sizes, times, marker='o')\n",
    "plt.xlabel('Number of Samples (k)')\n",
    "plt.ylabel('Time Taken (seconds)')\n",
    "plt.title('Time vs. Sample Size')\n",
    "plt.show()\n",
    "\n",
    "# Does the plot support your thoughts regarding the scale?\n",
    "# Approximately how many product requests can be processed in one second?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made it! We solved the issue regarding product access times for Quickmart. You can now make an index proposal with your head held high and put it into production. However, this is beyond our exercise ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Quickmart has noted that not only with the increased number of customers and requests into the product table everything got slower, but also with the number of products they have in the database. They didn't put a lot of effort to remove discontinued products from the products table. Now they have asked themselves whether it is possible to estimate how much slower the system becomes with an increasing number of products for the slow and fast variants.\n",
    "\n",
    "To answer this question, we need to fix the number of requests into the data table (.e.g. 10000 for the slow access an 200000 for the quick one) and vary the number of products from the products table. Please try to estimate the influence (the scaling regarding time) when varying the size of the products table (e.g. 1000, 5000, 10000, 15000, 20000, 25000, 30000, 35000).\n",
    "\n",
    "Additional Task: Provide a plot with the x-axis corresponding to the number of products and the y axis corresponding to the time it took for the request of products. How does time scale with an increased number of products?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas we can also use SQL to query data. In this exercise, we want to try out a few basic SQL operations to get an idea of its power. To do that, we download the data from the pandas visualization exercise and we are all setup to run some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the missing package pandasql in colab\n",
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to download the dataframe\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib\n",
    "# for animation\n",
    "matplotlib.rcParams['animation.embed_limit'] = 50.0\n",
    "\n",
    "# urls\n",
    "URL_LIFE_EXPECTANCY = 'https://small-waffle.gapminder.org/fasttrack/master/b2642a8?_select_key@=country&=time;&value@=lex;;&from=datapoints&where_'\n",
    "URL_GDP = 'https://small-waffle.gapminder.org/fasttrack/master/b2642a8?_select_key@=country&=time;&value@=gdp/_pcap;;&from=datapoints&where_'\n",
    "URL_POPULATION = 'https://small-waffle.gapminder.org/fasttrack/master/b2642a8?_select_key@=country&=time;&value@=pop;;&from=datapoints&where_'\n",
    "URL_COUNTRY = 'https://restcountries.com/v3.1/all'\n",
    "\n",
    "def read_url(url):\n",
    "    data = requests.get(url).json()\n",
    "    return pd.DataFrame(data['rows'], columns=data['header'])\n",
    "\n",
    "def retrieve_country_information():\n",
    "    response = requests.get(URL_COUNTRY)\n",
    "    cs = pd.DataFrame(response.json())[['name', 'cca3', 'region']]\n",
    "    country = cs.name.map(lambda x: x['common'])\n",
    "    cs = cs.assign(country=country)\n",
    "    cs = cs.drop(columns='name')\n",
    "    cs = cs.rename(columns={'cca3': 'country_short'})\n",
    "    cs = cs.assign(country_short = cs.country_short.str.lower())\n",
    "    return cs\n",
    "\n",
    "def get_data():\n",
    "    keys = ['country', 'time']\n",
    "    df_life_expectancy = read_url(URL_LIFE_EXPECTANCY)\n",
    "    df_gdp = read_url(URL_GDP)\n",
    "    df_population = read_url(URL_POPULATION)\n",
    "    df_countries = retrieve_country_information()\n",
    "    df = (df_life_expectancy\n",
    "          .merge(df_gdp, on=keys)\n",
    "          .merge(df_population, on=keys)\n",
    "          .rename(columns={'country': 'country_short'})\n",
    "          .merge(df_countries, on='country_short', how='left')\n",
    "        [['time', 'country', 'region', 'gdp_pcap', 'pop', 'lex']])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasql import sqldf\n",
    "\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1: display the first ten rows from the pandas dataframe 'data' using pandas\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2: Using SQL, try to display the same table\n",
    "# hint: you can call an SQL statement using sqldf(\"SQL QUERY HERE\")\n",
    "sqldf(\"SELECT * FROM data LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 3\n",
    "# Using SQL, how many rows do we have?\n",
    "sqldf(\"SELECT count(*) FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4\n",
    "# Using SQL, how many rows do we have when filtering out the predictions (time > 2024)\n",
    "sqldf('''\n",
    "      SELECT count(*)\n",
    "      FROM data\n",
    "      WHERE time <= 2024\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 5\n",
    "# Using SQL, what is the number of countries within a region?\n",
    "sqldf('''\n",
    "    SELECT\n",
    "      region,\n",
    "      count(country)\n",
    "    FROM data\n",
    "    GROUP BY region\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 6\n",
    "# Did you realize that the number of countries in a region is quite high? Let's refine the task:\n",
    "# Using SQL, what is the number of distinct countries within a region?\n",
    "sqldf('''\n",
    "    SELECT\n",
    "        region,\n",
    "        count(distinct country)\n",
    "    FROM data\n",
    "    GROUP BY region\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msb_lecture-tqAp08WJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
